{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "415891b4",
   "metadata": {},
   "source": [
    "# TLC Hourly Demand Processing (Optimized for Memory)\n",
    "This notebook processes TLC Parquet files efficiently by:\n",
    "- Processing files in chunks (avoids loading all data at once)\n",
    "- Saving intermediate hourly aggregates\n",
    "- Combining results and adding features\n",
    "- Saving final partitioned Parquet files\n",
    "- Optional cleanup of intermediate files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad26972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import holidays\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae7b3c",
   "metadata": {},
   "source": [
    "## Step 2: Setup Directories\n",
    "We create separate directories for intermediate and final outputs:\n",
    "- `final_dir` for final partitioned Parquet files\n",
    "- `intermediate_dir` for intermediate hourly aggregates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd05ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"data/daily_demand_partitioned\"\n",
    "final_dir = Path(output_dir) / \"final\"\n",
    "intermediate_dir = Path(output_dir) / \"intermediate_location\"\n",
    "final_dir.mkdir(parents=True, exist_ok=True)\n",
    "intermediate_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaf28e4",
   "metadata": {},
   "source": [
    "## Step 3: Infer Date Range from Filenames\n",
    "We infer the date range from filenames to filter data later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5774316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Date range inferred: 2023-01-01 00:00:00 to 2025-06-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Input files\n",
    "input_pattern = \"data/tlc_*/**/*.parquet\"\n",
    "input_files = glob.glob(input_pattern, recursive=True)\n",
    "\n",
    "# Infer date range\n",
    "dates = []\n",
    "for f in input_files:\n",
    "    match = re.search(r'(\\d{4})[-_]?(\\d{2})', f)\n",
    "    if match:\n",
    "        year, month = int(match.group(1)), int(match.group(2))\n",
    "        dates.append(pd.Timestamp(year=year, month=month, day=1))\n",
    "start_date, end_date = min(dates), max(dates) + pd.offsets.MonthEnd(1)\n",
    "\n",
    "print(f\"✅ Date range inferred: {start_date} to {end_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e506e63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 105314015\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "\n",
    "input_files = glob.glob(\"data/tlc_*/**/*.parquet\", recursive=True)\n",
    "\n",
    "total_rows = 0\n",
    "for f in input_files:\n",
    "    metadata = pq.ParquetFile(f).metadata\n",
    "    total_rows += metadata.num_rows\n",
    "\n",
    "print(\"Total rows:\", total_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76c542",
   "metadata": {},
   "source": [
    "## Step 4: Process Files in Chunks and Save Intermediate Aggregates\n",
    "We process each file individually, normalize schema, compute hourly aggregates, and save intermediate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f029b1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 60/60 [03:17<00:00,  3.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process files in chunks -> DAILY aggregates\n",
    "for f in tqdm(input_files, desc=\"Processing files\"):\n",
    "    chunk = pd.read_parquet(f)\n",
    "\n",
    "    # Normalize schema\n",
    "    if 'tpep_pickup_datetime' in chunk.columns:\n",
    "        chunk.rename(columns={'tpep_pickup_datetime': 'pickup_datetime'}, inplace=True)\n",
    "    elif 'lpep_pickup_datetime' in chunk.columns:\n",
    "        chunk.rename(columns={'lpep_pickup_datetime': 'pickup_datetime'}, inplace=True)\n",
    "\n",
    "    # Keep necessary columns\n",
    "    chunk = chunk[['pickup_datetime', 'PULocationID', 'DOLocationID']]\n",
    "    chunk['pickup_datetime'] = pd.to_datetime(chunk['pickup_datetime'])\n",
    "\n",
    "    # Aggregate by DATE\n",
    "    chunk['date'] = chunk['pickup_datetime'].dt.date\n",
    "    daily = chunk.groupby(['date', 'PULocationID', 'DOLocationID']).size().reset_index(name='rides')\n",
    "\n",
    "    # Save intermediate\n",
    "    daily.to_parquet(intermediate_dir / f\"daily_{os.path.basename(f)}\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfc6c53",
   "metadata": {},
   "source": [
    "## Step 5: Combine Intermediate Results\n",
    "We combine all intermediate hourly aggregates and filter by date range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760a8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine intermediate results\n",
    "daily_files = glob.glob(str(intermediate_dir / \"daily_*.parquet\"))\n",
    "daily_demand = pd.concat([pd.read_parquet(f) for f in daily_files], ignore_index=True)\n",
    "daily_demand = daily_demand.groupby(['date', 'PULocationID', 'DOLocationID'], as_index=False)['rides'].sum()\n",
    "\n",
    "# Filter by date range\n",
    "daily_demand['date'] = pd.to_datetime(daily_demand['date'])\n",
    "daily_demand = daily_demand[(daily_demand['date'] >= start_date) & (daily_demand['date'] <= end_date)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75968ee0",
   "metadata": {},
   "source": [
    "## Step 6-9: Add Features (Time-based, Lag, Holiday, Weather)\n",
    "We add additional features for modeling and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a6b0263",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add features\n",
    "daily_demand['year'] = daily_demand['date'].dt.year.astype('int16')\n",
    "daily_demand['month'] = daily_demand['date'].dt.month.astype('int8')\n",
    "daily_demand['weekday'] = daily_demand['date'].dt.weekday.astype('int8')\n",
    "\n",
    "# Lag features\n",
    "daily_demand.sort_values('date', inplace=True)\n",
    "for lag in [1, 2, 7]:\n",
    "    daily_demand[f'lag_{lag}'] = daily_demand['rides'].shift(lag)\n",
    "\n",
    "# Holiday flag\n",
    "years = daily_demand['year'].unique()\n",
    "us_holidays = holidays.US(years=years)\n",
    "daily_demand['is_holiday'] = daily_demand['date'].dt.date.astype(str).isin(us_holidays).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16a06773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>rides</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_7</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>181</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  PULocationID  DOLocationID  rides  year  month  weekday  \\\n",
       "148  2023-01-01             1             1     37  2023      1        6   \n",
       "5562 2023-01-01           181            33      2  2023      1        6   \n",
       "\n",
       "      lag_1  lag_2  lag_7  is_holiday  \n",
       "148     NaN    NaN    NaN           0  \n",
       "5562   37.0    NaN    NaN           0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_demand.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91243e00",
   "metadata": {},
   "source": [
    "## Step 10: Save Final Partitioned Parquet Files\n",
    "We save the final dataset partitioned by year and month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f359e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Daily processing complete. Final data saved to data\\daily_demand_partitioned\\final\n"
     ]
    }
   ],
   "source": [
    "# Save final partitioned Parquet\n",
    "daily_demand.dropna(inplace=True)\n",
    "daily_demand.iloc[:-1, :].to_parquet(final_dir, engine=\"pyarrow\", partition_cols=['year', 'month'], index=False)\n",
    "print(f\"✅ Daily processing complete. Final data saved to {final_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa4cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
