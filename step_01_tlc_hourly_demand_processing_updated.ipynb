{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "415891b4",
   "metadata": {},
   "source": [
    "# TLC Hourly Demand Processing (Optimized for Memory)\n",
    "This notebook processes TLC Parquet files efficiently by:\n",
    "- Processing files in chunks (avoids loading all data at once)\n",
    "- Saving intermediate hourly aggregates\n",
    "- Combining results and adding features\n",
    "- Saving final partitioned Parquet files\n",
    "- Optional cleanup of intermediate files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad26972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import holidays\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae7b3c",
   "metadata": {},
   "source": [
    "## Step 2: Setup Directories\n",
    "We create separate directories for intermediate and final outputs:\n",
    "- `final_dir` for final partitioned Parquet files\n",
    "- `intermediate_dir` for intermediate hourly aggregates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd05ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"data/hourly_demand_partitioned\"\n",
    "final_dir = Path(output_dir) / \"final\"\n",
    "intermediate_dir = Path(output_dir) / \"intermediate\"\n",
    "\n",
    "final_dir.mkdir(parents=True, exist_ok=True)\n",
    "intermediate_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaf28e4",
   "metadata": {},
   "source": [
    "## Step 3: Infer Date Range from Filenames\n",
    "We infer the date range from filenames to filter data later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c741922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Date range inferred: 2023-01-01 00:00:00 to 2025-06-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "input_pattern = \"data/tlc_*/**/*.parquet\"\n",
    "input_files = glob.glob(input_pattern, recursive=True)\n",
    "\n",
    "dates = []\n",
    "for f in input_files:\n",
    "    match = re.search(r'(\\d{4})[-_]?(\\d{2})', f)\n",
    "    if match:\n",
    "        year, month = int(match.group(1)), int(match.group(2))\n",
    "        dates.append(pd.Timestamp(year=year, month=month, day=1))\n",
    "\n",
    "if dates:\n",
    "    start_date = min(dates)\n",
    "    end_date = max(dates) + pd.offsets.MonthEnd(1)\n",
    "else:\n",
    "    raise ValueError(\"‚ùå No valid dates found in filenames\")\n",
    "\n",
    "print(f\"‚úÖ Date range inferred: {start_date} to {end_date}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76c542",
   "metadata": {},
   "source": [
    "## Step 4: Process Files in Chunks and Save Intermediate Aggregates\n",
    "We process each file individually, normalize schema, compute hourly aggregates, and save intermediate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f029b1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [02:47<00:00,  2.80s/it]\n"
     ]
    }
   ],
   "source": [
    "for f in tqdm(input_files, desc=\"Processing files\"):\n",
    "    chunk = pd.read_parquet(f)\n",
    "\n",
    "    # Normalize schema\n",
    "    if 'tpep_pickup_datetime' in chunk.columns:\n",
    "        chunk.rename(columns={'tpep_pickup_datetime': 'pickup_datetime',\n",
    "                              'tpep_dropoff_datetime': 'dropoff_datetime'}, inplace=True)\n",
    "    elif 'lpep_pickup_datetime' in chunk.columns:\n",
    "        chunk.rename(columns={'lpep_pickup_datetime': 'pickup_datetime',\n",
    "                              'lpep_dropoff_datetime': 'dropoff_datetime'}, inplace=True)\n",
    "\n",
    "    # Keep only necessary columns\n",
    "    chunk = chunk[['pickup_datetime', 'dropoff_datetime']]\n",
    "\n",
    "    # Convert datetime\n",
    "    chunk['pickup_datetime'] = pd.to_datetime(chunk['pickup_datetime'])\n",
    "    chunk['dropoff_datetime'] = pd.to_datetime(chunk['dropoff_datetime'])\n",
    "\n",
    "    # Compute trip duration and filter invalid trips\n",
    "    chunk['trip_duration_min'] = (chunk['dropoff_datetime'] - chunk['pickup_datetime']).dt.total_seconds() / 60\n",
    "    chunk = chunk[chunk['trip_duration_min'] > 0]\n",
    "\n",
    "    # Aggregate hourly demand for this file\n",
    "    chunk['date_hour'] = chunk['pickup_datetime'].dt.floor('h')\n",
    "    hourly = chunk.groupby('date_hour').size().reset_index(name='rides')\n",
    "\n",
    "    # Save intermediate result\n",
    "    hourly.to_parquet(intermediate_dir / f\"hourly_{os.path.basename(f)}\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfc6c53",
   "metadata": {},
   "source": [
    "## Step 5: Combine Intermediate Results\n",
    "We combine all intermediate hourly aggregates and filter by date range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760a8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_files = glob.glob(str(intermediate_dir / \"hourly_*.parquet\"))\n",
    "hourly_demand = pd.concat([pd.read_parquet(f) for f in hourly_files], ignore_index=True)\n",
    "\n",
    "# Aggregate again to merge overlapping hours\n",
    "hourly_demand = hourly_demand.groupby('date_hour', as_index=False)['rides'].sum()\n",
    "\n",
    "# Filter by inferred date range\n",
    "hourly_demand = hourly_demand[(hourly_demand['date_hour'] >= start_date) & (hourly_demand['date_hour'] <= end_date)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75968ee0",
   "metadata": {},
   "source": [
    "## Step 6-9: Add Features (Time-based, Lag, Holiday, Weather)\n",
    "We add additional features for modeling and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a6b0263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based features\n",
    "hourly_demand['year'] = hourly_demand['date_hour'].dt.year.astype('int16')\n",
    "hourly_demand['month'] = hourly_demand['date_hour'].dt.month.astype('int8')\n",
    "hourly_demand['hour'] = hourly_demand['date_hour'].dt.hour.astype('int8')\n",
    "hourly_demand['weekday'] = hourly_demand['date_hour'].dt.weekday.astype('int8')\n",
    "\n",
    "# Lag features\n",
    "for lag in [1, 2, 24]:\n",
    "    hourly_demand[f'lag_{lag}'] = hourly_demand['rides'].shift(lag)\n",
    "\n",
    "hourly_demand.dropna(inplace=True)\n",
    "\n",
    "# Holiday flag\n",
    "years = hourly_demand['year'].unique()\n",
    "us_holidays = holidays.US(years=years)\n",
    "hourly_demand['is_holiday'] = hourly_demand['date_hour'].dt.date.astype(str).isin(us_holidays).astype('int8')\n",
    "\n",
    "# Merge weather data if available\n",
    "weather_files = glob.glob(\"data/weather*.parquet\")\n",
    "if weather_files:\n",
    "    weather = pd.concat([pd.read_parquet(f) for f in weather_files], ignore_index=True)\n",
    "    weather['date_hour'] = pd.to_datetime(weather['datetime']).dt.floor('h')\n",
    "    weather = weather[(weather['date_hour'] >= start_date) & (weather['date_hour'] <= end_date)]\n",
    "    expected_cols = ['temp', 'precip']\n",
    "    available_cols = [col for col in expected_cols if col in weather.columns]\n",
    "    hourly_demand = hourly_demand.merge(weather[['date_hour'] + available_cols], on='date_hour', how='left')\n",
    "    for col in available_cols:\n",
    "        hourly_demand[col] = hourly_demand[col].fillna(method='ffill')\n",
    "else:\n",
    "    hourly_demand['temp'] = np.nan\n",
    "    hourly_demand['precip'] = np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91243e00",
   "metadata": {},
   "source": [
    "## Step 10: Save Final Partitioned Parquet Files\n",
    "We save the final dataset partitioned by year and month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f359e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processing complete. Final data saved to data\\hourly_demand_partitioned\\final\n"
     ]
    }
   ],
   "source": [
    "hourly_demand.iloc[:-1, :].to_parquet(final_dir, engine=\"pyarrow\", partition_cols=['year', 'month'], index=False)\n",
    "print(f\"‚úÖ Processing complete. Final data saved to {final_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db49c4",
   "metadata": {},
   "source": [
    "## Step 11: Optional Cleanup of Intermediate Files\n",
    "Set `cleanup_intermediate = True` to remove intermediate files after processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f62b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_intermediate = False  # Set to False to keep intermediate files\n",
    "if cleanup_intermediate:\n",
    "    for f in hourly_files:\n",
    "        os.remove(f)\n",
    "    print(f\"üßπ Intermediate files cleaned up from {intermediate_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa4cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uplift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
